{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe891f4-73c9-42e6-9b22-0ab7fa708d4a",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 7 по теме \"Рекуррентные сети для обработки последовательностей\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef895452-87d2-4a7b-af09-cb38f2971ad4",
   "metadata": {},
   "source": [
    "1. Обучите нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979466c-b380-40a9-b731-059c62631eb0",
   "metadata": {},
   "source": [
    "Данная задача во многом повторяет предыдущее практическое задание, поэтому  \n",
    "скопируем оттуда часть, отвечающую за подготовку данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd54421-4165-40e0-b397-5ef08610225e",
   "metadata": {},
   "source": [
    "Загрузим необходимые библиотеки и данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7705e3ce-9708-4d37-b61e-0cd1e2080689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc542fe-0987-4385-9d31-46610afebf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78630fd7-9c48-497a-8ee7-e7bf2b9f85f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0   @user when a father is dysfunctional and is s...\n",
       "2       0  @user @user thanks for #lyft credit i can't us...\n",
       "3       0                                bihday your majesty\n",
       "4       0  #model   i love u take with u all the time in ...\n",
       "5       0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../Lesson_6/data/train.csv', index_col='id')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794cecc-65b1-402c-8ed0-2f54dc53b897",
   "metadata": {},
   "source": [
    "<ins>Описание датасета:</ins>  \n",
    "The objective of this task is to detect hate speech in tweets.  \n",
    "For the sake of simplicity, we say a tweet contains hate speech  \n",
    "if it has a racist or sexist sentiment associated with it.  \n",
    "So, the task is to classify racist or sexist tweets from other tweets.  \n",
    "  \n",
    "Formally, given a training sample of tweets and labels, where label '1'  \n",
    "denotes the tweet is racist/sexist and label '0' denotes the tweet is  \n",
    "not racist/sexist, your objective is to predict the labels on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5517e58-b627-42c0-b478-8a17c182e1ca",
   "metadata": {},
   "source": [
    "Таким образом, нам нужно будет искать твиты, которые содержат  \n",
    "расистский или сексистский смысл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be0f1d8-adba-4079-82bf-2e5930054892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17197, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('../Lesson_6/data/test.csv', index_col='id')\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6c2aa-82f1-448d-82fd-18ad31cfc811",
   "metadata": {},
   "source": [
    "Так как тестовые данные не содержат меток, то будем использовать только  \n",
    "трейн для обучения и валидации, чтобы можно было оценить качество модели.  \n",
    "Посмотрим на баланс классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25687d9d-8f24-448e-8c1e-97358509dc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b43ed91-ceb2-489d-8c19-9350840365df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.256021409455842"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()[0] / df_train['label'].value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54d6e0-6f17-47fb-be4a-758a07365d5c",
   "metadata": {},
   "source": [
    "Как часто бывает в подобных задачах, мы имеем большой дисбаланс классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f6789-be70-43c3-9f8e-56e0e5788a42",
   "metadata": {},
   "source": [
    "Сделаем разбивку на трейн и валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feab13e5-8694-40ec-a97b-4fc080946a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 2), (6393, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=RANDOM_STATE, \n",
    "                                    stratify=df_train['label'])\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deda1b8-4601-432c-b73c-9122eeae159e",
   "metadata": {},
   "source": [
    "Сделаем подготовку текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcf6393-ec0c-4505-ad1c-53be82956018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/shkin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/shkin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafab76b-204c-44a1-8061-259ab655b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)\n",
    "# Не будем очищать текст от апострофов, заменим их потом на пробелы,\n",
    "# т.к. встроенные в nltk английские стопслова и так потом отфильтруют лишнее\n",
    "puncts = puncts - {\"'\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26536f51-a53e-4b2f-9135-a2a8cbe3baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = ''.join(char for char in txt if char not in puncts) # очистка от пунктуации\n",
    "    txt = txt.replace(\"'\", \" \")\n",
    "    txt = txt.lower().split()\n",
    "    txt = [word for word in txt if word.isalpha()] # очистка от символов и цифр\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt] # лемматизация\n",
    "    txt = [word for word in txt if word not in stopwords.words('english')] # очистка от стопслов\n",
    "    return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053f7aa3-650d-4722-87f7-33f7d3ce0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 25569/25569 [00:20<00:00, 1245.45it/s]\n",
      "100%|█████████████████████████████████████| 6393/6393 [00:04<00:00, 1317.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c600d11-9fa8-4fed-b348-f380b98d2e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14553</th>\n",
       "      <td>0</td>\n",
       "      <td>user amazing wait see going cantwait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>0</td>\n",
       "      <td>wait new user trailer gamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>0</td>\n",
       "      <td>thriving iam positive affirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>0</td>\n",
       "      <td>happy new user book lil upset page faded user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>arrive cold rainy english noh first time back ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "14553      0               user amazing wait see going cantwait\n",
       "2563       0                        wait new user trailer gamer\n",
       "12125      0                  thriving iam positive affirmation\n",
       "6326       0  happy new user book lil upset page faded user ...\n",
       "3996       0  arrive cold rainy english noh first time back ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b36837-7d9a-4f0f-8570-534bab02d451",
   "metadata": {},
   "source": [
    "Подготовим общий корпус текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee076e4a-fcca-4422-9439-56e35dedd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ''.join(df_train['tweet'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07648845-698e-4d68-a82f-d9fb76126f9f",
   "metadata": {},
   "source": [
    "Сделаем токенизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffac299-d572-4d58-a149-7ef600d1d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'amazing', 'wait', 'see', 'going']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c704e6-7759-43c5-88e3-77fd1f0ac065",
   "metadata": {},
   "source": [
    "Создадим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15561f3e-c03c-474c-a2a6-742bd8b00839",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 4000\n",
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e85b4ece-c828-46d2-813d-40c0ef138cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens)\n",
    "tokens_top = [items[0] for items in dist.most_common(MAX_WORDS - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0298c17a-a867-4179-808d-ce0a31572045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'day', 'love', 'u', 'amp', 'like', 'life', 'happy', 'get', 'wa']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7772776-b47c-40ec-9795-01b58061afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {word: count for count, word in dict(enumerate(tokens_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84bb53-5ef9-4dec-b831-d9e3691e37f2",
   "metadata": {},
   "source": [
    "Переведём твиты в набор индексов, добавим паддинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c833215f-919b-4702-b925-d1f3a4eacb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(txt, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "285af485-572e-47f3-a662-5f139fa1c986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 40), (6393, 40))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_train['tweet'].values])\n",
    "X_val = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_val['tweet'].values])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e9b042f-08b6-4146-beb9-559737dd7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальная строка: found beautiful one bedroom double stall garage patio amp huge kitchen signed lease wait move\n",
      "Обработанная строка: [ 172   51   19 1233 3015 3475    5  777 1537 1538   68  694    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Оригинальная строка: {df_train['tweet'].iloc[5]}\")\n",
    "print(f\"Обработанная строка: {X_train[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee80d18-be4b-42a6-a1da-fa48999f4c47",
   "metadata": {},
   "source": [
    "Инициализируем рекуррентную нейросеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5627c1bd-3183-4230-a36d-985511a15ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=128, out_dim=64, use_last=True, threshold=0.5, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n",
    "        self.gru = nn.GRU(embedding_dim, out_dim, batch_first=True) \n",
    "        self.linear = nn.Linear(out_dim, num_classes)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        self.use_last = use_last\n",
    "        \n",
    "    def forward(self, x):                          \n",
    "        x = self.embedding(x)\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.gru(x)\n",
    "           \n",
    "        if self.use_last:\n",
    "            x = x[:,-1,:]\n",
    "        else:\n",
    "            x = torch.mean(x[:,:], dim=1)\n",
    "            \n",
    "        x = self.dp(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = torch.IntTensor(x).to(device)\n",
    "        x = self.forward(x)\n",
    "        x = torch.squeeze((x > self.threshold).int())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d42e3-7f61-4380-8ca0-7fc2e1c9b57c",
   "metadata": {},
   "source": [
    "Посмотрим структуру сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3913313-b3b1-4bce-a39c-9c4b2c6c353f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 40, 128]              256,000\n",
       "├─Dropout: 1-2                           [1, 40, 128]              --\n",
       "├─GRU: 1-3                               [1, 40, 64]               37,248\n",
       "├─Dropout: 1-4                           [1, 64]                   --\n",
       "├─Linear: 1-5                            [1, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 293,313\n",
       "Trainable params: 293,313\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 1.17\n",
       "Estimated Total Size (MB): 1.23\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Net(), input_data=torch.IntTensor(X_train[np.newaxis, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af16551-6da0-4ffd-8a14-13da833b310c",
   "metadata": {},
   "source": [
    "Подготовим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7dee0fc-7c3f-4620-ba8a-870465e47cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aeb691b-40be-49fa-aeb2-2aa595d50091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a22066e7-f8b1-4ee8-bb2b-b22548cfbf5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(RANDOM_STATE)\n",
    "\n",
    "train_dataset = DataWrapper(X_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(X_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5987398-2d94-4c20-ab12-3b3fa4d736be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33680c1-375e-49d4-b4ff-e87970012f15",
   "metadata": {
    "tags": []
   },
   "source": [
    "Напишем код сети. Учитывая дисбаланс классов, метрика accuracy нам  \n",
    "не подходит. Вместо неё будем использовать F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893b9e0e-fae6-4e52-bea9-19786d1ae658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nn(epochs=5, embedding_dim=128, hidden_size=64, lr=1e-3, threshold=0.5, use_last=True, return_model=False):\n",
    "\n",
    "    torch.random.manual_seed(RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    net = Net(vocab_size=MAX_WORDS, embedding_dim=embedding_dim, \n",
    "              out_dim=hidden_size, use_last=use_last, threshold=threshold).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = np.array([])\n",
    "        test_losses = np.array([])\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses = np.append(train_losses, loss.item())\n",
    "\n",
    "            net.eval()\n",
    "            outputs = torch.squeeze((net(inputs) > threshold).int())\n",
    "\n",
    "            tp += ((labels == 1) & (outputs == 1)).sum().item()\n",
    "            tn += ((labels == 0) & (outputs == 0)).sum().item()\n",
    "            fp += ((labels == 0) & (outputs == 1)).sum().item()\n",
    "            fn += ((labels == 1) & (outputs == 0)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "              f'Loss: {train_losses.mean():.3f}. ' \\\n",
    "              f'F1-score: {f1_score:.3f}', end='. ')\n",
    "\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                test_losses = np.append(test_losses, loss.item())\n",
    "\n",
    "                tp += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                tn += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "                fp += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                fn += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Test loss: {test_losses.mean():.3f}. Test F1-score: {f1_score:.3f}. Precision: {precision:.3f}. Recall: {recall:.3f}')\n",
    "\n",
    "    print('Training is finished!')\n",
    "    if return_model:\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc47f3d-97d3-403a-b105-04ec098e6af4",
   "metadata": {},
   "source": [
    "Обучим модель на 70 эпохах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22684ecf-7212-4480-9053-993f5ac652f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70]. Loss: 0.456. F1-score: 0.098. Test loss: 0.265. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [2/70]. Loss: 0.263. F1-score: 0.000. Test loss: 0.252. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [3/70]. Loss: 0.260. F1-score: 0.000. Test loss: 0.247. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [4/70]. Loss: 0.250. F1-score: 0.000. Test loss: 0.228. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [5/70]. Loss: 0.219. F1-score: 0.000. Test loss: 0.187. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [6/70]. Loss: 0.192. F1-score: 0.040. Test loss: 0.170. Test F1-score: 0.081. Precision: 1.000. Recall: 0.042\n",
      "Epoch [7/70]. Loss: 0.176. F1-score: 0.377. Test loss: 0.156. Test F1-score: 0.498. Precision: 0.767. Recall: 0.368\n",
      "Epoch [8/70]. Loss: 0.164. F1-score: 0.559. Test loss: 0.149. Test F1-score: 0.545. Precision: 0.716. Recall: 0.440\n",
      "Epoch [9/70]. Loss: 0.152. F1-score: 0.636. Test loss: 0.147. Test F1-score: 0.546. Precision: 0.775. Recall: 0.422\n",
      "Epoch [10/70]. Loss: 0.144. F1-score: 0.656. Test loss: 0.143. Test F1-score: 0.555. Precision: 0.752. Recall: 0.440\n",
      "Epoch [11/70]. Loss: 0.141. F1-score: 0.671. Test loss: 0.136. Test F1-score: 0.590. Precision: 0.720. Recall: 0.500\n",
      "Epoch [12/70]. Loss: 0.137. F1-score: 0.696. Test loss: 0.137. Test F1-score: 0.564. Precision: 0.802. Recall: 0.435\n",
      "Epoch [13/70]. Loss: 0.128. F1-score: 0.716. Test loss: 0.142. Test F1-score: 0.566. Precision: 0.825. Recall: 0.431\n",
      "Epoch [14/70]. Loss: 0.127. F1-score: 0.730. Test loss: 0.134. Test F1-score: 0.592. Precision: 0.766. Recall: 0.482\n",
      "Epoch [15/70]. Loss: 0.120. F1-score: 0.741. Test loss: 0.136. Test F1-score: 0.603. Precision: 0.788. Recall: 0.489\n",
      "Epoch [16/70]. Loss: 0.119. F1-score: 0.761. Test loss: 0.135. Test F1-score: 0.603. Precision: 0.798. Recall: 0.484\n",
      "Epoch [17/70]. Loss: 0.117. F1-score: 0.768. Test loss: 0.137. Test F1-score: 0.616. Precision: 0.774. Recall: 0.511\n",
      "Epoch [18/70]. Loss: 0.113. F1-score: 0.784. Test loss: 0.135. Test F1-score: 0.624. Precision: 0.786. Recall: 0.518\n",
      "Epoch [19/70]. Loss: 0.108. F1-score: 0.787. Test loss: 0.135. Test F1-score: 0.622. Precision: 0.805. Recall: 0.507\n",
      "Epoch [20/70]. Loss: 0.107. F1-score: 0.791. Test loss: 0.129. Test F1-score: 0.626. Precision: 0.811. Recall: 0.509\n",
      "Epoch [21/70]. Loss: 0.102. F1-score: 0.809. Test loss: 0.132. Test F1-score: 0.643. Precision: 0.768. Recall: 0.554\n",
      "Epoch [22/70]. Loss: 0.103. F1-score: 0.812. Test loss: 0.136. Test F1-score: 0.642. Precision: 0.767. Recall: 0.551\n",
      "Epoch [23/70]. Loss: 0.097. F1-score: 0.818. Test loss: 0.138. Test F1-score: 0.636. Precision: 0.786. Recall: 0.533\n",
      "Epoch [24/70]. Loss: 0.098. F1-score: 0.828. Test loss: 0.133. Test F1-score: 0.647. Precision: 0.788. Recall: 0.549\n",
      "Epoch [25/70]. Loss: 0.095. F1-score: 0.833. Test loss: 0.130. Test F1-score: 0.647. Precision: 0.811. Recall: 0.538\n",
      "Epoch [26/70]. Loss: 0.093. F1-score: 0.836. Test loss: 0.135. Test F1-score: 0.648. Precision: 0.780. Recall: 0.554\n",
      "Epoch [27/70]. Loss: 0.091. F1-score: 0.843. Test loss: 0.133. Test F1-score: 0.640. Precision: 0.832. Recall: 0.520\n",
      "Epoch [28/70]. Loss: 0.093. F1-score: 0.843. Test loss: 0.143. Test F1-score: 0.660. Precision: 0.816. Recall: 0.554\n",
      "Epoch [29/70]. Loss: 0.088. F1-score: 0.859. Test loss: 0.130. Test F1-score: 0.659. Precision: 0.799. Recall: 0.560\n",
      "Epoch [30/70]. Loss: 0.086. F1-score: 0.862. Test loss: 0.137. Test F1-score: 0.668. Precision: 0.770. Recall: 0.589\n",
      "Epoch [31/70]. Loss: 0.083. F1-score: 0.867. Test loss: 0.140. Test F1-score: 0.661. Precision: 0.816. Recall: 0.556\n",
      "Epoch [32/70]. Loss: 0.084. F1-score: 0.870. Test loss: 0.134. Test F1-score: 0.657. Precision: 0.827. Recall: 0.545\n",
      "Epoch [33/70]. Loss: 0.083. F1-score: 0.876. Test loss: 0.138. Test F1-score: 0.665. Precision: 0.774. Recall: 0.583\n",
      "Epoch [34/70]. Loss: 0.083. F1-score: 0.879. Test loss: 0.129. Test F1-score: 0.660. Precision: 0.793. Recall: 0.565\n",
      "Epoch [35/70]. Loss: 0.079. F1-score: 0.887. Test loss: 0.137. Test F1-score: 0.660. Precision: 0.797. Recall: 0.562\n",
      "Epoch [36/70]. Loss: 0.077. F1-score: 0.890. Test loss: 0.140. Test F1-score: 0.665. Precision: 0.759. Recall: 0.592\n",
      "Epoch [37/70]. Loss: 0.076. F1-score: 0.893. Test loss: 0.148. Test F1-score: 0.669. Precision: 0.770. Recall: 0.592\n",
      "Epoch [38/70]. Loss: 0.073. F1-score: 0.893. Test loss: 0.145. Test F1-score: 0.670. Precision: 0.797. Recall: 0.578\n",
      "Epoch [39/70]. Loss: 0.073. F1-score: 0.899. Test loss: 0.144. Test F1-score: 0.664. Precision: 0.815. Recall: 0.560\n",
      "Epoch [40/70]. Loss: 0.075. F1-score: 0.902. Test loss: 0.140. Test F1-score: 0.663. Precision: 0.778. Recall: 0.578\n",
      "Epoch [41/70]. Loss: 0.073. F1-score: 0.910. Test loss: 0.136. Test F1-score: 0.654. Precision: 0.780. Recall: 0.562\n",
      "Epoch [42/70]. Loss: 0.073. F1-score: 0.910. Test loss: 0.146. Test F1-score: 0.661. Precision: 0.713. Recall: 0.616\n",
      "Epoch [43/70]. Loss: 0.069. F1-score: 0.917. Test loss: 0.144. Test F1-score: 0.660. Precision: 0.769. Recall: 0.578\n",
      "Epoch [44/70]. Loss: 0.068. F1-score: 0.917. Test loss: 0.143. Test F1-score: 0.663. Precision: 0.770. Recall: 0.583\n",
      "Epoch [45/70]. Loss: 0.065. F1-score: 0.922. Test loss: 0.151. Test F1-score: 0.661. Precision: 0.732. Recall: 0.603\n",
      "Epoch [46/70]. Loss: 0.068. F1-score: 0.924. Test loss: 0.145. Test F1-score: 0.660. Precision: 0.733. Recall: 0.600\n",
      "Epoch [47/70]. Loss: 0.066. F1-score: 0.926. Test loss: 0.144. Test F1-score: 0.666. Precision: 0.721. Recall: 0.618\n",
      "Epoch [48/70]. Loss: 0.066. F1-score: 0.927. Test loss: 0.151. Test F1-score: 0.672. Precision: 0.758. Recall: 0.603\n",
      "Epoch [49/70]. Loss: 0.064. F1-score: 0.925. Test loss: 0.147. Test F1-score: 0.667. Precision: 0.800. Recall: 0.571\n",
      "Epoch [50/70]. Loss: 0.062. F1-score: 0.931. Test loss: 0.149. Test F1-score: 0.663. Precision: 0.716. Recall: 0.618\n",
      "Epoch [51/70]. Loss: 0.064. F1-score: 0.929. Test loss: 0.150. Test F1-score: 0.672. Precision: 0.785. Recall: 0.587\n",
      "Epoch [52/70]. Loss: 0.061. F1-score: 0.935. Test loss: 0.150. Test F1-score: 0.668. Precision: 0.770. Recall: 0.589\n",
      "Epoch [53/70]. Loss: 0.064. F1-score: 0.936. Test loss: 0.149. Test F1-score: 0.662. Precision: 0.792. Recall: 0.569\n",
      "Epoch [54/70]. Loss: 0.061. F1-score: 0.939. Test loss: 0.160. Test F1-score: 0.670. Precision: 0.747. Recall: 0.607\n",
      "Epoch [55/70]. Loss: 0.059. F1-score: 0.944. Test loss: 0.154. Test F1-score: 0.664. Precision: 0.761. Recall: 0.589\n",
      "Epoch [56/70]. Loss: 0.056. F1-score: 0.948. Test loss: 0.162. Test F1-score: 0.667. Precision: 0.728. Recall: 0.616\n",
      "Epoch [57/70]. Loss: 0.059. F1-score: 0.947. Test loss: 0.154. Test F1-score: 0.671. Precision: 0.791. Recall: 0.583\n",
      "Epoch [58/70]. Loss: 0.055. F1-score: 0.951. Test loss: 0.148. Test F1-score: 0.668. Precision: 0.781. Recall: 0.583\n",
      "Epoch [59/70]. Loss: 0.057. F1-score: 0.953. Test loss: 0.161. Test F1-score: 0.675. Precision: 0.767. Recall: 0.603\n",
      "Epoch [60/70]. Loss: 0.053. F1-score: 0.953. Test loss: 0.162. Test F1-score: 0.663. Precision: 0.774. Recall: 0.580\n",
      "Epoch [61/70]. Loss: 0.057. F1-score: 0.955. Test loss: 0.162. Test F1-score: 0.668. Precision: 0.772. Recall: 0.589\n",
      "Epoch [62/70]. Loss: 0.054. F1-score: 0.951. Test loss: 0.177. Test F1-score: 0.663. Precision: 0.705. Recall: 0.625\n",
      "Epoch [63/70]. Loss: 0.052. F1-score: 0.959. Test loss: 0.167. Test F1-score: 0.663. Precision: 0.720. Recall: 0.614\n",
      "Epoch [64/70]. Loss: 0.056. F1-score: 0.960. Test loss: 0.166. Test F1-score: 0.666. Precision: 0.777. Recall: 0.583\n",
      "Epoch [65/70]. Loss: 0.050. F1-score: 0.961. Test loss: 0.164. Test F1-score: 0.668. Precision: 0.733. Recall: 0.614\n",
      "Epoch [66/70]. Loss: 0.050. F1-score: 0.964. Test loss: 0.166. Test F1-score: 0.668. Precision: 0.770. Recall: 0.589\n",
      "Epoch [67/70]. Loss: 0.049. F1-score: 0.962. Test loss: 0.168. Test F1-score: 0.671. Precision: 0.760. Recall: 0.600\n",
      "Epoch [68/70]. Loss: 0.047. F1-score: 0.961. Test loss: 0.167. Test F1-score: 0.673. Precision: 0.787. Recall: 0.587\n",
      "Epoch [69/70]. Loss: 0.050. F1-score: 0.964. Test loss: 0.166. Test F1-score: 0.672. Precision: 0.747. Recall: 0.612\n",
      "Epoch [70/70]. Loss: 0.049. F1-score: 0.965. Test loss: 0.157. Test F1-score: 0.681. Precision: 0.779. Recall: 0.605\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(epochs=70, use_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e7499-0b30-4d54-81a9-abfb39b27a1d",
   "metadata": {},
   "source": [
    "Модель GRU показала результат лучше, чем 1D свёртки из прошлого практического  \n",
    "задания, F1-score выше где-то на 8%. Также она оказалась чуть лучше, чем модель  \n",
    "LSTM (выявлено в результате перебора гиперпараметров). По логам видно, что  \n",
    "переобучение снова присутствует. Интересно, что модель GRU положительно отреагировала  \n",
    "на увеличение размера словаря и длины последовательности, тогда как свёрточная сеть  \n",
    "никак на это не реагировала.  \n",
    "Финальную модель обучим на 30 эпохах, где у нас относительно малый тест лосс  \n",
    "и относительно высокая метрика. Так же, как и в прошлый раз, снизим порог  \n",
    "классификации для получения более высокого Recall, который важен в нашей задаче:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b5d718-a6c5-4b3f-bee8-d4ee974d6385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]. Loss: 0.456. F1-score: 0.124. Test loss: 0.265. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [2/30]. Loss: 0.263. F1-score: 0.000. Test loss: 0.252. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [3/30]. Loss: 0.260. F1-score: 0.000. Test loss: 0.247. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [4/30]. Loss: 0.250. F1-score: 0.000. Test loss: 0.228. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [5/30]. Loss: 0.219. F1-score: 0.347. Test loss: 0.187. Test F1-score: 0.453. Precision: 0.505. Recall: 0.411\n",
      "Epoch [6/30]. Loss: 0.192. F1-score: 0.534. Test loss: 0.170. Test F1-score: 0.508. Precision: 0.552. Recall: 0.471\n",
      "Epoch [7/30]. Loss: 0.176. F1-score: 0.602. Test loss: 0.156. Test F1-score: 0.556. Precision: 0.544. Recall: 0.569\n",
      "Epoch [8/30]. Loss: 0.164. F1-score: 0.643. Test loss: 0.149. Test F1-score: 0.579. Precision: 0.589. Recall: 0.569\n",
      "Epoch [9/30]. Loss: 0.152. F1-score: 0.672. Test loss: 0.147. Test F1-score: 0.586. Precision: 0.625. Recall: 0.551\n",
      "Epoch [10/30]. Loss: 0.144. F1-score: 0.699. Test loss: 0.143. Test F1-score: 0.594. Precision: 0.685. Recall: 0.525\n",
      "Epoch [11/30]. Loss: 0.141. F1-score: 0.707. Test loss: 0.136. Test F1-score: 0.615. Precision: 0.611. Recall: 0.618\n",
      "Epoch [12/30]. Loss: 0.137. F1-score: 0.722. Test loss: 0.137. Test F1-score: 0.603. Precision: 0.698. Recall: 0.531\n",
      "Epoch [13/30]. Loss: 0.128. F1-score: 0.740. Test loss: 0.142. Test F1-score: 0.603. Precision: 0.720. Recall: 0.518\n",
      "Epoch [14/30]. Loss: 0.127. F1-score: 0.754. Test loss: 0.134. Test F1-score: 0.623. Precision: 0.662. Recall: 0.589\n",
      "Epoch [15/30]. Loss: 0.120. F1-score: 0.770. Test loss: 0.136. Test F1-score: 0.625. Precision: 0.663. Recall: 0.592\n",
      "Epoch [16/30]. Loss: 0.119. F1-score: 0.782. Test loss: 0.135. Test F1-score: 0.632. Precision: 0.701. Recall: 0.576\n",
      "Epoch [17/30]. Loss: 0.117. F1-score: 0.789. Test loss: 0.137. Test F1-score: 0.630. Precision: 0.674. Recall: 0.592\n",
      "Epoch [18/30]. Loss: 0.113. F1-score: 0.797. Test loss: 0.135. Test F1-score: 0.639. Precision: 0.696. Recall: 0.592\n",
      "Epoch [19/30]. Loss: 0.108. F1-score: 0.806. Test loss: 0.135. Test F1-score: 0.641. Precision: 0.713. Recall: 0.583\n",
      "Epoch [20/30]. Loss: 0.107. F1-score: 0.809. Test loss: 0.129. Test F1-score: 0.641. Precision: 0.691. Recall: 0.598\n",
      "Epoch [21/30]. Loss: 0.102. F1-score: 0.825. Test loss: 0.132. Test F1-score: 0.652. Precision: 0.661. Recall: 0.643\n",
      "Epoch [22/30]. Loss: 0.103. F1-score: 0.828. Test loss: 0.136. Test F1-score: 0.650. Precision: 0.655. Recall: 0.645\n",
      "Epoch [23/30]. Loss: 0.097. F1-score: 0.835. Test loss: 0.138. Test F1-score: 0.651. Precision: 0.703. Recall: 0.607\n",
      "Epoch [24/30]. Loss: 0.098. F1-score: 0.838. Test loss: 0.133. Test F1-score: 0.641. Precision: 0.685. Recall: 0.603\n",
      "Epoch [25/30]. Loss: 0.095. F1-score: 0.843. Test loss: 0.130. Test F1-score: 0.645. Precision: 0.687. Recall: 0.607\n",
      "Epoch [26/30]. Loss: 0.093. F1-score: 0.848. Test loss: 0.135. Test F1-score: 0.656. Precision: 0.684. Recall: 0.629\n",
      "Epoch [27/30]. Loss: 0.091. F1-score: 0.856. Test loss: 0.133. Test F1-score: 0.667. Precision: 0.748. Recall: 0.603\n",
      "Epoch [28/30]. Loss: 0.093. F1-score: 0.859. Test loss: 0.143. Test F1-score: 0.658. Precision: 0.718. Recall: 0.607\n",
      "Epoch [29/30]. Loss: 0.088. F1-score: 0.860. Test loss: 0.130. Test F1-score: 0.648. Precision: 0.664. Recall: 0.634\n",
      "Epoch [30/30]. Loss: 0.086. F1-score: 0.866. Test loss: 0.137. Test F1-score: 0.650. Precision: 0.647. Recall: 0.652\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "my_net = train_nn(epochs=30, threshold=0.25, return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc67ef6-42fe-40ef-87f4-a0a3548ee55c",
   "metadata": {},
   "source": [
    "По сравнению с предыдущим решением на свёртках, мы находим почти столько же  \n",
    "оскорбительных твитов (64-65%). Но показатель точности теперь выше на 21%  \n",
    "(65% против 44%). В общем, эти результаты всё равно являются не очень хорошими для  \n",
    "готовой модели. Снова будем считать, что основной причиной является недостаток данных  \n",
    "(25 тысяч примеров в обучающей выборке, из которых всего 1800 положительного  \n",
    "класса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9e3a630-cc01-4c9b-a88d-d9ce771a0833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23775\n",
       "1     1794\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e83959-8b24-4eb0-919d-1ee1941b969c",
   "metadata": {},
   "source": [
    "Предсказание модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ee2f98c-5b38-4f04-8267-f42cd00c7ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.predict(X_val[np.newaxis, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077e643-77b1-4bfc-812d-a6891b6c6297",
   "metadata": {},
   "source": [
    "Задача выполнена."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
