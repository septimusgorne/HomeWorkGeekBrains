{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 3. Dataset, Dataloader, BatchNorm, Dropout, Оптимизация\n",
    "\n",
    "- Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)\n",
    "- Обернуть его в Dataloader\n",
    "- Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "- Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "import warnings\n",
    "import math\n",
    "from random import shuffle\n",
    "from tools import paralell_execution, ar_split_eq_cpu\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CaliforniaDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, init_dataset, init_target, transform=None):\n",
    "        self._base_dataset = torch.from_numpy(init_dataset).type(torch.float)\n",
    "        self._base_targets = torch.from_numpy(init_target).type(torch.float)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self._base_dataset[idx]\n",
    "        target = self._base_targets[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "      \n",
    "        return features, target\n",
    "\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        if self.activation == \"leaky_relu\":\n",
    "            return F.leaky_relu(x)\n",
    "        raise RuntimeError\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, hidden_dim, 'leaky_relu')\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp = nn.Dropout(0.15)\n",
    "        self.fc2 = Perceptron(hidden_dim, 1, 'leaky_relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего комбинаций параметров: 12\n",
      "Лучшая r2_score: -0.021 при параметрах {'optim_func': <class 'torch.optim.rmsprop.RMSprop'>, 'lr': 0.005, 'epochs': 55}\n"
     ]
    }
   ],
   "source": [
    "# комбинации параметров нейросети \n",
    "params = []\n",
    "\n",
    "for optim_func in [optim.Adam, optim.RMSprop, optim.SGD]:\n",
    "    for lr in [1e-3, 5e-3]: # 1e-3, 5e-3, 1e-2\n",
    "        for epochs in [15, 55]:\n",
    "            params.append({\n",
    "                'optim_func': optim_func, \n",
    "                'lr': lr, \n",
    "                'epochs': epochs, \n",
    "            })\n",
    "\n",
    "shuffle(params)\n",
    "# params = params[:1]\n",
    "print(f'Всего комбинаций параметров: {len(params)}')\n",
    "\n",
    "def train(params):\n",
    "    ret = []\n",
    "    for param in params:\n",
    "        data = datasets.fetch_california_housing()\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=0.2, random_state=42)\n",
    "        \n",
    "        train_dataset = CaliforniaDataset(X_train, y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=150, shuffle=False)\n",
    "        \n",
    "        test_dataset = CaliforniaDataset(X_test, y_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "        \n",
    "        net = FeedForward(8, 8)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "        optimizer = param['optim_func'](net.parameters(), lr=param['lr'])\n",
    "        for epoch in range(param['epochs']):  \n",
    "            running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "    \n",
    "            for i, data in enumerate(train_loader):\n",
    "                fets, target = data[0], data[1]\n",
    "                \n",
    "                optimizer.zero_grad() # обнуляем градиент\n",
    "    \n",
    "                outputs = net(fets)\n",
    "                loss = criterion(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                running_loss += loss.item()\n",
    "                running_items += len(target)\n",
    "    \n",
    "                predict = outputs.data.numpy()\n",
    "                tr_target = target.view(target.shape[0], 1).numpy()\n",
    "                r2 += r2_score(tr_target, predict)\n",
    "                running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "        # вычисляем точность\n",
    "        net.eval()\n",
    "        data = list(test_loader)[0]\n",
    "        test_outputs = net(data[0])\n",
    "        test_predict = test_outputs.data.numpy()\n",
    "        te_target = data[1].view(data[1].shape[0], 1)\n",
    "        r2_result = r2_score(te_target, test_predict)\n",
    "        net.train()\n",
    "        \n",
    "        ret.append({\n",
    "            'r2': r2_result,\n",
    "            'param': param,\n",
    "            'net': net\n",
    "        })\n",
    "    return ret\n",
    "\n",
    "args_parallel = []\n",
    "for param_chunk in ar_split_eq_cpu(params):\n",
    "    args_parallel.append(param_chunk)\n",
    "\n",
    "# запускаем процесс параллельных вычислений\n",
    "res = paralell_execution(func=train,\n",
    "                         arg=args_parallel,\n",
    "                         method='multiprocessing')\n",
    "\n",
    "best_r2 = None # точность лучшей модели\n",
    "best_param = {} # параметры лучшей модели\n",
    "best_net = None # лучшая модель\n",
    "for chunk in res: # объединяем результаты со всех ядер\n",
    "    for row in chunk:\n",
    "        if best_r2 is None or abs(row['r2']) < abs(best_r2):\n",
    "            best_r2 = row['r2']\n",
    "            best_param = row['param']\n",
    "            best_net = row['net']\n",
    "\n",
    "print(f'Лучшая r2_score: {round(best_r2, 3)} при параметрах {best_param}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
