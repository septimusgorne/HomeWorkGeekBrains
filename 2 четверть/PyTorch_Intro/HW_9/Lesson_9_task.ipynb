{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1997d11-3b41-44d1-8bcc-bc272876927a",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 9 по теме \"Трансформер\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ca8af-19d5-4d6d-a342-d8cd01d070ce",
   "metadata": {},
   "source": [
    "1. Возьмите готовую модель из https://huggingface.co/models для классификации сентимента текста.\n",
    "2. Сделайте предсказания на всем df_val. Посчитайте метрику качества.\n",
    "3. Дообучите эту модель на df_train. Посчитайте метрику качества на df_val."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89e777-94cd-45ee-a1d6-e123e8bbb6e7",
   "metadata": {},
   "source": [
    "Загрузим библиотеки и датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e768f6d-2278-4f31-bbf8-91163d05ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6483011d-3397-48f7-9a0c-607a654a7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2834b142-e2ab-4fad-96df-df3b1a9cfdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  class\n",
       "id                                                          \n",
       "0   @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../../Теория/Lesson_9/train.csv', index_col='id')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9df654-423a-4bbe-83a7-96f326424440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('../../Теория/Lesson_9/val.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c6e4c-d934-46ab-b8b1-08ec090a1daa",
   "metadata": {},
   "source": [
    "В качестве модели возьмём fine-tuned версию модели distilBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83fac36-b9bd-41e6-adf7-06782dff219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'philschmid/distilbert-base-multilingual-cased-sentiment-2'\n",
    "\n",
    "model = pipeline('text-classification', model=model_name, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff631a-fff4-4825-811f-d996e08de45d",
   "metadata": {},
   "source": [
    "Сделаем предсказания на валидации. Модель возвращает метки трёх  \n",
    "классов: позитивного, негативного и нейтрального. Нейтральный будем  \n",
    "считать негативным, так как в результате теста этот способ показал  \n",
    "себя лучше, чем если считать его позитивным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f075c2-8dac-4391-b649-e5d36bce060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "    pred = model(text)[0]['label']\n",
    "    match pred:\n",
    "        case 'positive':\n",
    "            pred = 1\n",
    "        case 'negative':\n",
    "            pred = 0\n",
    "        case 'neutral':\n",
    "            pred = 0\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27a27ab-d4e8-4394-a82b-6a5cbb793442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 2/22683 [00:00<1:22:03,  4.61it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████| 22683/22683 [01:13<00:00, 308.05it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "predictions = df_val['text'].progress_apply(lambda x: make_prediction(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d471716-6bb6-4d39-afb5-57a0fbfd40c3",
   "metadata": {},
   "source": [
    "Оценим точность предсказаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb085086-46bb-4b9b-8bb1-7a8e7bff61cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8086231979896839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions == df_val['class']).sum() / len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdce4b-b52c-402b-ab23-436f1528955d",
   "metadata": {},
   "source": [
    "Результат составил почти 81%, что заметно превышает результаты  \n",
    "моделей CNN и RNN, полученные на предыдущих уроках.  \n",
    "Теперь дообучим эту модель под наш датасет на два класса.  \n",
    "Сначала обернём датасет в даталоадер. В процессе будем делать  \n",
    "токенизацию, как делали на уроке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29274d63-b4de-4d12-a1e8-fda07e7ac23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, txts, labels):\n",
    "        self._labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._txts = [self.tokenizer(text, padding='max_length', max_length=70,\n",
    "                                     truncation=True, return_tensors=\"pt\")\n",
    "                      for text in txts]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._txts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._txts[index], self._labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71294c02-f607-49fd-a890-a0bdfec6a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(RANDOM_STATE)\n",
    "\n",
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values\n",
    "\n",
    "train_dataset = TwitterDataset(df_train['text'], y_train)\n",
    "valid_dataset = TwitterDataset(df_val['text'], y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cf6ff8-1277-417d-8767-446ee5f21646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e79886-943e-47f8-9c45-a126d2b25a18",
   "metadata": {},
   "source": [
    "Посмотрим на модель. При загрузке укажем, что у нас будет два класса, а не три:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11af77f-fadd-4c67-9737-6977ae68073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at philschmid/distilbert-base-multilingual-cased-sentiment-2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0af470-ca68-4ebd-983c-9269fdc68d60",
   "metadata": {},
   "source": [
    "Будем обучать два последних линейных слоя. Посмотрим на количество параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e49fbe-e55e-4944-970e-7006a7f7f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              91,812,096\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 135,326,210\n",
       "Trainable params: 135,326,210\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c9944-d399-4cc8-98c0-7bab870d9afa",
   "metadata": {},
   "source": [
    "Обучить нужно примерно 592 тыс. параметров из 135 млн.  \n",
    "Проведём обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d51c5481-c3b3-46e1-9bc1-ca6a6e3b8460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at philschmid/distilbert-base-multilingual-cased-sentiment-2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████| 5671/5671 [08:39<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.423         | Train Accuracy:  0.804         | Val Loss:  0.384         | Val Accuracy:  0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5671/5671 [08:03<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.412         | Train Accuracy:  0.811         | Val Loss:  0.368         | Val Accuracy:  0.833\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "model = model.to(device)\n",
    "\n",
    "param_groups = [\n",
    "    {'params': model.pre_classifier.parameters(), 'lr': 0.001},\n",
    "    {'params': model.classifier.parameters(), 'lr': 0.001}\n",
    "]\n",
    "optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "for epoch_num in range(2):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_loader):\n",
    "        model.train()\n",
    "        mask = train_input['attention_mask'].to(device)\n",
    "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "        train_label = train_label.to(device)\n",
    "\n",
    "        output = model(input_ids=input_id, attention_mask=mask, labels=train_label)\n",
    "                \n",
    "        batch_loss = output.loss\n",
    "        total_loss_train += batch_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (output.logits.argmax(dim=1) == train_label).sum().item()\n",
    "        total_acc_train += acc\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_val, total_acc_val = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_input, val_label in valid_loader:\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_ids=input_id, attention_mask=mask, labels=val_label)\n",
    "\n",
    "            batch_loss = output.loss\n",
    "            total_loss_val += batch_loss\n",
    "\n",
    "            acc = (output.logits.argmax(dim=1) == val_label).sum().item()\n",
    "            total_acc_val += acc\n",
    "            \n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_loader): .3f} \\\n",
    "        | Train Accuracy: {total_acc_train / len(train_dataset): .3f} \\\n",
    "        | Val Loss: {total_loss_val / len(valid_loader): .3f} \\\n",
    "        | Val Accuracy: {total_acc_val / len(valid_dataset): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4e6f6-402f-4b96-b0ae-2b405e7d9948",
   "metadata": {},
   "source": [
    "Дообучение позволило увеличить точность модели на 2.5%.  \n",
    "Можно ещё добавить эпох и поподбирать гиперпараметры  \n",
    "оптимизатора, но одна эпоха обучается 8 минут, и подбор  \n",
    "займёт прилично времени. Считаю задание выполненным."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
